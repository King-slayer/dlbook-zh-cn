\chapter{应用}
\label{ch:applications}

在这一章，我们描述如何使用\gls*{dl}来解决在计算机视觉、语音识别、自然语言处理和其
它商业性领域的应用。首先，我们讨论最重要的人工智能应用所需的大规模神经网络实现。
接下来，我们回顾几个\gls*{dl}已经被用于解决问题的特殊应用领域。虽然\gls*{dl}的一
个目标是设计能够解决各种各样任务的算法，到目前为止还需要一定程度的专业化。例如，
视觉任务每个样本需要处理大量输入特征（像素）。语言任务每个输入特征需要对大量可能
值（词汇表中的单词）建模。

\section{大规模深度学习}
\label{sec:large_scale_deep_learning}

\gls*{dl}是基于联结主义的哲学：虽然单个的生物神经元或者机器学习模型中的单个特征并
% See https://en.wikipedia.org/wiki/Connectionism
不智能，大量神经元或者特征的共同作用可以表现出智能行为。强调这一事实非常重要，神
经元的数量必须是\textbf{大量的}。主导改进神经网络准确性和任务复杂度~——~那些从 80
年代至今解决的任务~——~的一个关键因素，是我们使用的网络规模上引人注目的增长。正如
我们在~\ref{subsec:increasing_model_sizes} 节中看到的，网络规模在过去三十年间以指
数方式增长，然而人工神经网络仅仅和昆虫的神经系统一样大。

因为神经网络的规模是非常重要的，\gls*{dl}需要高性能的硬件和软件基础。

\subsection{快速的 CPU 实现}
\label{subsec:fast_cpu_implementations}

传统上，神经网络利用一台机器的 CPU 训练。今天，这种方法被认为是不够的。我们现在主
要使用 GPU 运算或者多机联网的 CPU。在移到这些昂贵的装置前，研究人员努力证明 CPU
无法完成神经网络需要的很高的计算工作量。

如何执行高效数值计算的 CPU 代码的描述超出了这本书的范围，但我们在这里强调，仔细为
特定的 CPU 家族的实现可以产生较大的改进。例如，在 2011 年，最好的 CPU 在使用定点
运算时，运行神经网络的工作量要快于使用浮点运算。通过创造一个仔细调试的定点实
现，\citet{Vanhoucke-et-al-2011} 取得了超过一个强大的浮点系统的三倍提速。每个新
的 CPU 型号有不同的性能特点，所以有时浮点实现也能够更快。重要的原则是，对数值计算
程序的仔细的专门处理，可以获得很大的回报。其他策略，除了选择是否使用定点或浮点，
还包括优化数据结构，以避免高速缓存块未命中（cache misses）和使用矢量指令。许多机
器学习的研究人员忽视了这些实现细节，但当具体实现的性能限制模型的大小时，模型的准
确性受到影响。

\subsection{GPU 实现}
\label{subsec:gpu_implementations}

大部分现代神经网络的实现基于图形处理单元。图形处理单元（GPU）是最初为图形应用开发
的专门的硬件组件。视频游戏系统的消费者市场刺激了图形处理硬件的发展。好的视频游戏
系统所需的性能特性结果发现对神经网络同样有益。

视频游戏渲染需要并行快速地执行很多操作。角色和环境模型以三维的顶点坐标列表的形式
指定。图形卡必须执行许多顶点的矩阵乘除，同时并行地将这些三维坐标转换到二维的屏幕
坐标。图形卡必须在每个像素执行许多计算，同时并行地确定每个像素的颜色。在这两种情
况下，计算都相当简单，和一个 CPU 通常遇到的计算工作量相比，并不涉及大量的分支。例
如，在相同的刚性物体中的每个顶点会被相同的矩阵相乘；没有必要每个顶点用 {\serif
  if} 声明求值来确定乘以哪个矩阵。这些计算彼此间也完全无关，这样可以很容易并行化。
这些计算也涉及处理大量的内存缓存，包括描述每个渲染物体纹理（颜色图案）的位图。结
合在一起，这导致图形卡被设计成具有高度的并行性和高内存带宽，其代价是相对于传
统CPU 有一个更低的时钟速度和更少的分支能力。

神经网络算法需要和上面描述的实时图形算法相同的性能特性。神经网络通常涉及数量庞大
的参数、激活值和梯度值的缓存，其中每个都必须在每一步的训练中被完整更新。这些缓存
大到足够超出传统的桌面计算机的高速缓冲区（cache）而崩溃。GPU，由于它们的高内存带
宽，提供了一个超过 CPU 的不可抗拒的优势。神经网络训练算法通常不涉及很多分支或者复
杂控制，所以它们适用于 GPU 硬件。既然神经网络能被分成多个单独的“神经元”，这些“神
经元”在相同层中能彼此独立地被处理，神经网络很容易从 GPU 计算的并行性获益。

GPU 硬件最初是如此特殊，它只能用于图形任务。随着时间的过去，GPU 硬件变得更灵活了，
允许定制子程序用于变换定点坐标或者分配颜色给像素。原则上，没有要求这些像素值要实
际基于渲染任务。这些 GPU 能通过将计算所得的输出写入到像素值的缓存而被用于特殊的计
算。\citet{Steinkrau2005} 在一块 GPU 上实现了一个两层的全连接神经网络并宣告了一个
超过他们基于 CPU 基线的三倍提速。此后不久，\citet{chellapilla:inria-00112631}演示
了同样的技术能被用于加速有监督的卷积网络。

用于神经网络训练的图形卡在\textbf{通用 GPU}发明后得到爆炸性的普及。这些 GP-GPU 能
够执行任意代码，不仅仅是渲染子程序。NVIDIA 的 CUDA 可编程语言提供了一种类 C 语言
的方式编写任意代码。有了它们相对方便的编程模型、大规模并行结构和高内存带宽，现
在GP-GPU 提供了一个用于神经网络编程的理想平台。这一平台在面世之后迅速被深度学习研
究人员采用 \citep{RainaICML09,Ciresan-2010}。

为 GP-GPU 编写有效的代码仍然是一项困难的任务，最好留给专家。在 GPU 上获取好的性能
需要的技术和在 CPU 上使用的非常不同。例如，好的基于 CPU 的代码通常被设计为尽可能
从高速缓冲区（cache）读取信息。在 GPU 上，大部分可写的内存位置并没有被高速缓冲，
所以它其实可以更快来两次计算相同值，而不是一次计算然后从内存读回。GPU 代码本质上
也是多线程的，不同的线程必须相互间周密协调。例如，如果内存操作可以\textbf{被合
  并}（\textit{coalesced}），它们可以被更快执行。合并的读写发生在几个线程的每一个
能同时读或写一个它们需要值的时候，作为单次内存处理的一部分。不同的 GPU 模型能够合
并不同类型的读写模式。通常，在 $n$ 个线程中，线程 $i$ 获取内存的 $i + j$ 字节
时~——~其中 $j$ 是 $2$ 的某个倍数~——~，内存操作很容易合并。确切的规格随着 GPU 型号
差异而不同。另一个普遍的对 GPU 的考虑是确保一组中的每一个线程同时执行相同的指令。
这意味着在 GPU 上分支会很困难。线程被分进小的组，称为\emph{\gls{warps}}。一
个\gls*{warps}中的每个线程在每个周期中执行相同的指令，所以如果在同一
个\gls*{warps}中的不同线程需要执行不同的代码路径，这些不同的代码路径必须按次序遍
历，而不是以平行方式。

由于编写高性能 GPU 代码的困难性，研究人员应该构造他们的工作流程来避免为了测试新的
模型或算法必须编写新的 GPU 代码。通常，可以通过构建一个高性能运算~——~例如卷积和矩
阵乘法~——~的软件库，然后以调用这些运算库的形式指定模型。例如，机器学习
库 Pylearn2 \citep{Goodfellow+al-ICML2013-small} 以调用 Theano
\citep{bergstra+al:2010-scipy-short,Bastien-2012} 和 cuda-convnet
\citep{Krizhevsky2010tr} 的形式指定所有它的机器学习算法，它们提供了这些高性能运算。
这种分解方法也可以缓解多种硬件支持。例如，同样的 Theano 程序可以运行
在 CPU 或 GPU上，不需要改变任何自身对 Theano 的调用。其它库，例如 TensorFlow
\citep{tensorflow} 和 Torch \citep{Collobert-AISTATS2011} 提供了类似的特性。

\subsection{大规模分布式实现}
\label{subsec:large_scale_distributed_implementations}

在许多情况下，在一台机器上可用的计算资源是不够的。因此，我们想要在多台机器间分配
训练和推断结果的工作负担。

分配推断结果的工作简单，因为每个我们想要处理的能被单独的机器运行。这称
为\textbf{数据并行}。

也可能取得\textbf{模型并行}，其中多台机器在单个数据点上一起工作，每台机器运行模型
的不同部分。这对于推断结果和训练都很灵活。

训练期间的数据并行处理有点难。我们可以增加用于单个 \gls{SGD} 步骤的小批量数据的规
模，但是通常我们在性能优化方面得到更少的线性回报。允许多台机器并行地计算多个梯度
下降步骤会更好。不幸的是，梯度下降的标准定义是一个完全按次序的算法：在步骤 $t$ 的
梯度是一个参数由步骤 $t-1$ 产生的函数。

这可以通过使用\textbf{异步随机梯度下降}\citep{NIPS2011_4390}来解决。在这种方法中，
% FIXME: cannot find (Bengio et al., 2001)
几个处理器核心共享表示参数的内存。每个核心在不锁定的情况下读取参数，计算一个梯度，
然后在不锁定的情况下递增这些参数。这减少了每个梯度下降步骤得出改进的平均量，因为
一些核心复写了彼此的进度，但是那些步骤产生的增加的速率使得学习进程整体上更
快。\citet{NIPS2012_4687}首创了这用于梯度下降的无锁方法的多机实现，其中参数由一台
参数服务器管理，而不是存储在共享内存中。分布式异步梯度下降仍然是训练大规模深度网
络的主要策略，并且被大多数行业内主要的深度学习群体使用\citep{186212,
  DBLP:journals/corr/WuYSDS15}。大学院校的深度学习研究人员通常不能负担得起同样规
模的分布式学习系统，但是一些研究人员已经专注于如何用在大学环境中可取得的相对低成
本的硬件构建分布式网络\citep{coates2013deep}。

\subsection{模型压缩}
\label{subsec:model_compression}

在许多商业应用中，在一个机器学习模型中花费低的时间和内存开销用于运行推断，要比花
费低的时间和内存开销用于训练，要重要得多。对于那些不需要个性化的应用，训练一个模
型一次，然后将它部署由数十亿用户使用，这是可能的。在许多情况下，最终用户比开发者
有更多的资源约束。例如，一种情况可能是用一个强大的计算机集群训练一个语音识别网络，
然后把它部署到移动电话。

一个减少推断开销的关键策略是\textbf{模型压缩}\citep{Ungar:2006:1150402}。模型压
缩的基本思想是用一个更小的模型~——~它需要更少的内存和运行时间来存储和求值~——~代替
原始的代价昂贵的模型。

模型压缩适用于当原始模型的规模主要由因为避免\gls{overfitting}的需要而驱动。在大
多数情况下，具有最低的\gls{generalization_error}的模型由几个独立地训练过的模型组
合而成。计算所有 $n$ 个组合中成员的值代价昂贵。有时候，甚至单个的模型，如果规模
很大，泛化得更好（例如，如果使用\gls{dropout}来规范化）。

这些大的模型学习某个函数 $f(\pmb{x})$，但是这样做使用比必要的任务更多的参数。它
们的规模是必要的，仅仅因为数量有限的训练样本。一旦我们拟合了这个函数
$f(\pmb{x})$，我们能简单地通过在随机取样的点 $\pmb{x}$ 上应用 $f$，来生成一个包
含无限多样本的训练集。为了最有效地利用新的、小模型的能力，最好从一个类似于实际测
试输入~——~这些输入会在后面提供给模型~——~的分布中取样新的 $\pmb{x}$ 点。这可以通
过使训练样本出错，或者从一个在原始训练集训练的、可生成的模型绘制点来完成。

另外，可以只在原始训练点上训练较小的模型，但训练它复制模型的其它特征，如它后面的
在错误分类上的分布。

\subsection{动态结构}
\label{subsec:dynamic_structure}

通常，加速数据处理系统的一个策略是构建在图中有\textbf{动态结构}~——~这里的图描述需
要处理一个输入的计算~——~的系统。数据处理系统能够动态地确定许多神经网络的哪个子集
应该在给定的输入上运行。单独的神经网络也可以在内部通过确定特征（隐藏单元）的哪个
子集计算来自输入的给定信息来表现出动态结构。这种神经网络内动态结构的形式有时候被
称为\textbf{条件计算}(Bengio, 2013; Bengio et al., 2013b)。既然许多架构中的组件可
以只和少量的可能输入相关，通过计算这些仅仅需要时计算的特征，系统能运行得更快。

动态结构是一种基本的计算机科学原理，广泛应用于整个软件工程学科。应用于神经网络的
动态结构的最简单的版本，是在确定某组神经网络（或其他机器学习模型）的哪个子集应该
应用于一个特定的输入的基础上。

在分类器中加速推断的一个历史悠久的策略，是使用一个分类器的\textbf{级联}。当目标是
检测是否存在一个特殊对象（或事件）时，可以使用级联策略。要确定该对象是存在的，我
们必须使用一个具有高容量的复杂精巧的分类器，运行它代价昂贵。然而，因为对象是特殊
的，我们通常能使用少得多的计算来舍弃不包含对象的输入。在这些情况下，我们可以训练
一系列的分类器。序列中的第一层分类器有低的容量，并被训练为具有高召回率。换句话说，
它们被训练为确保我们没有错误地舍弃对象存在时的输入。最后的分类器被训练为具有高精
度。在测试时，我们通过按序列地运行推断，一旦任何级联中的元素舍弃任何样本就立即抛
弃它。总体而言，这使我们有很高的信心能够验证的对象的存在，使用一个高容量的模型，
但不强迫我们为每一个样本付出充分推断的代价。级联有 2 种不同的方式可以实现高容量。
一种方法是使级联的后面成员各自具有很高的容量。在这种情况下，整个系统作为一个整体，
显然具有很高的容量，因为它的一些单独成员是这样。也可能制定这样一个级联，其中每个
单独的模型有低容量，但是整个系统因为是由许多小的模型组成而有高容量。Viola and
Jones (2001) 使用了一个\gls{bdt}的级联实现一个适用于手持数字相机的快速和鲁棒的人
脸探测器。他们的分类器用本质上是个滑动窗口~——~其中许多窗口被检测，如果它们不包含
人脸则被舍弃~——~的方法来确定一个人脸的位置。级联的另一个版本使用初期的模型来实现
一种硬性的引人注意的机制：级联的初期成员确定一个物体的位置，而级联后期的成员在给
定的物体位置执行更进一步的处理。例如，谷歌使用一个两步的级联从街景影响中转录地址
编号，第一步使用一个机器学习模型确定地址编号的位置，然后用另一个转录
它(Goodfellow et al., 2014d)。

决策树本身就是一个动态结构的例子，因为树中的每个节点决定了它的哪些子数应该对每个
输入求值。实现深度学习和动态结构的结合，一个简单的方法是训练这样一个决策树，其中
每个节点使用一个神经网络来作出分解的判决\citep{guo1992classification}，虽然这通常
没有随着加速推断计算的首要目标完成。

本着同样的精神，可以使用一个称为\textbf{门限}的神经网络来从几个\textbf{专家网
  络}中的选择哪一个会被用于对给定的当前输入计算输出。

\subsection{深度网络的专用硬件实现}
\label{subsec:specialized_hardware_implementations_of_deep_networks}

\section{计算机视觉}
\label{sec:computer_vision}

计算机视觉历来是深度学习应用最为活跃的一个应用领域，因为视觉是一个对于人类和许多
动物天生的，但对计算机则是挑战性的任务\citep{ballard1983parallel}。许多对深度学习
算法的进行基准测量的最流行的任务，是以物体识别或者光学字符识别的形式。

计算机视觉是一个非常广泛的领域，涵盖了各种各样的处理图像的方式，和令人惊奇的多种
多样的应用。计算机视觉的应用范围从再现人的视觉能力、如识别人脸，到创造全新的视觉
能力的类别。作为后者的一个例子，最近的计算机视觉应用程序可以从感应自视频中可见物
体的振动识别声音波形\citep{Davis2014VisualMic}。大部分对计算机视觉的深度学习研究
并没有专注于这样奇异的应用 —— 扩展图像的领域，相反是一个小的 AI 核心，其目的是复
制人类的能力。大部分针对计算机视觉的深度学习被用于物体识别或检测一些形状，不管这
意味着报告在一幅图像中存在哪个物体，在图像中框选每一个物体来作注释，从一幅图像中
转录一系列符号，还是标记图像中的每个像素属于哪个物体。由于可生成的建模已经成为深
度学习研究的指导性原理，因此还有大量在图像综合分析上使用深度模型的工作。虽然图像
综合分析通常无中生有地被认为不是计算机视觉的尝试，有图像综合分析能力的模型通常可
用于图像复原 —— 一个涉及到修复图像中的缺陷或是从图像中移除物体的计算机视觉任务。

\subsection{预处理}
\label{subsec:preprocessing}

许多应用领域需要复杂的预处理，因为原始的输入以一种对很多深度学习架构来说难于表达
的形式。计算机视觉通常需要相对少的这种预处理。图像应该被标准化，使得它们的像素都
位于相同的、合理的范围内，例如 $[0,1]$ 或 $[-1,1]$。把位于 $[0,1]$ 的图像和位
于 $[0,255]$ 的图像混合在一起通常导致失败。格式化图像具有相同的尺度是唯一的一种严
格必要的预处理。许多计算机视觉的架构需要一个标准尺寸的图像，所以图像必须被裁剪或
缩放以适应这种大小。然而，即使这种调整并不总是严格要求。一些卷积模型接受不同大小
的输入和动态调整的混合区域的大小来保持恒定的输出尺寸 (Waibel et al.,1989)。其它卷
积模型具有大小可变的输出，自动调整输入的大小，例如去噪或者标记图像中每个像素的模
型\citep{Hadsell-RSS-07}。

数据集增强可能被看作是一种只对训练集预处理的方式。数据集增强是一个极好的方法，以
减少大多数计算机视觉模型的泛化误差。一个相关的适用于测试时间的想法是显示模型的许
多不同版本的相同的输入（例如，相同的图像，但在稍微不同的位置进行裁切），并对该模
型的不同实例进行投票来决定输出。后者的想法可以被解释为一个整体方法，并有助于减少
泛化误差。

其他各种预处理同时被施加到训练和测试集，这样的目的是用一个更规范的形式来表示每个
样本，以减少模型需要考虑的变体的数量。减少数据中的变体数量，既能够减少泛化误差，
也能减少需要拟合训练集的模型规模。更简单的任务可以通过更小的模型来解决，而更简单
的解决方案更容易推广。这种预处理通常被设计为移除某些类型的输入数据的变化，这些变
化对一个人类设计师是很容易形容，并有信心其和任务无关。当训练大数据集和大模型时，
这种预处理通常就没有必要了，最好只是让模型学习哪种变化性应该变成不变的。例如，用
于分类 ImageNet 的 AlexNet 系统只有一个预处理步骤：在每个像素的训练样本上减去平均
值\citep{Krizhevsky-2012}。

\subsubsection{对比度归一化}
\label{subsubsec:contrast_nomalization}

最明显的、对许多任务可以安全移除的变化来源之一，是图像中的对比度。对比度只是指一
幅图像中明暗像素之间的差异的大小。有很多方法来量化一幅图像的对比度。在深度学习的
环境中，对比度通常指一幅图像或者其中一块区域中像素的标准偏差。假设我们有一幅由一
个张量 $\mathsf{X} \in \mathbb{R}^{r \times c \times 3}$ 表示的图像，
由 $\mathsfit{X}_{i,j,1}$ 表示在第 $i$ 行第 $j$ 列的红色分量强
度，$\mathsfit{X}_{i,j,2}$ 表示绿色强度，$\mathsfit{X}_{i,j,3}$ 表示蓝色强度。那
么整个图像的对比度为：
\begin{equation}
  \sqrt{\frac{1}{3rc}\displaystyle\sum_{i=1}^r\sum_{j=1}^c\sum_{k=1}^3(\mathsfit{X}_{i,j,k}
    - \bar{\pmb{\mathsf{X}}})^2}
  \label{eq:12.1}
\end{equation}
这里 $\bar{\pmb{\mathsf{X}}}$ 是整个图像的平均光强度：
\begin{equation}
  \bar{\pmb{\mathsf{X}}} = \frac{1}{3rc}\displaystyle\sum_{i=1}^r\sum_{j=1}^c\sum_{k=1}^3\mathsfit{X}_{i,j,k}
  \label{eq:12.2}
\end{equation}

\emph{\gls{gcn}} (GCN) 的目的是防止图像有不同量的对比度，通过从每个图像中减去的平
均值，然后调整图像使得它的像素上的标准差等于常数
$s$。这个方法背后复杂的事实，是没有调整因子可以改变一幅零对比度的图像（所有像素都
有相同强度的图像）。具有非常低但是非零对比度的图像通常有很少的信息内容。除以真正
的标准偏差通常只在这样的情况下放大传感器的噪声或压缩伪影。这促使对\gls*{bias}引入
一个小的、正的规范化参数 $\lambda$，标准差的估计。或者，可以限制分母至少
为 $\epsilon$。给定一个输入图像 $\pmb{\mathsf{X}}$，GCN 生成一个输出图
像 $\pmb{\mathsf{X}}'$，以这样定义：
\begin{equation}
  \mathsfit{X}\,'_{i,j,k} = s \frac{
    \mathsfit{X}_{i,j,k} - \bar{\mathsfit{X}}
  }{
    \mathrm{max}\bigg\{\epsilon, \sqrt{\lambda + \frac{1}{3rc}\sum_{i=1}^r\sum_{j=1}^c\sum_{k=1}^3(\mathsfit{X}_{i,j,k} - \bar{\mathsfit{X}})^2}\bigg\}
  }
  \label{eq:12.3}
\end{equation}

由裁剪为感兴趣对象的大量图像组成的数据集，不太可能包含任何具有几乎恒定强度的图像。
在这种情况下，稳妥的办法是通过设定 $\lambda = 0$ 来实际上忽略小分母的问题，以及
设定 $\epsilon$ 为一个例如 $10^{(-8)}$ 的极小值来避免极端罕见的情况下被 $0$ 除。
这是被 Goodfellow et al. (2013a) 使用在 CIFAR-10 数据集上的方法。小的经过随机剪
裁的图像更可能有接近恒定的强度，使得冒进的规范化更有用。\citet{Coates2011} 在取
自 CIFAR-10 的小的、随机选择的图像块上使用了 $\epsilon = 0$ 和 $\lambda = 10$。

尺度参数 $s$ 通常可以被设置为 $1$，正如 \citet{Coates2011} 做的，或者选为使得每
一个单独像素有接近 $1$ 的、在样本上的标准偏差，正如 Goodfellow et al. (2013a) 所
做的。

方程~\ref{eq:12.3} 中的标准偏差仅仅是图像的 $L^2$ 归一化的重新调节（假设图像的平
  均值已经被移除）。以标准偏差的形式定义 GCN 比 $L^2$ 归一化更合适，因为标准偏差
包含像素数量的除数，所以基于标准偏差的 GCN 允许使用相同的 $s$ 而不管图像的大小。

\subsubsection{数据集增强}
\label{subsubsec:dataset_augmentation}

\section{语音识别}
\label{sec:speech_recognition}

\section{自然语言处理}
\label{sec:natural_language_processing}

\subsection{$n$ 元语法模型}
\label{subsec:ngrams}

\subsection{神经语言模型}
\label{subsec:neural_language_models}

\subsection{高维输出}
\label{high_dimentional_outputs}

