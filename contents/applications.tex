\chapter{应用}
\label{ch:applications}

在这一章，我们描述如何使用深度学习来解决在计算机视觉、语音识别、自然语言处理和其
它商业性领域的应用。首先，我们讨论最重要的人工智能应用所需的大规模神经网络实现。
接下来，我们回顾几个深度学习已经被用于解决问题的特殊应用领域。虽然深度学习的一个
目标是设计能够解决各种各样任务的算法，到目前为止还需要一定程度的专业化。例如，视
觉任务每个样本需要处理大量输入特征（像素）。语言任务每个输入特征需要对大量可能值
（词汇表中的单词）建模。

\section{大规模深度学习}
\label{sec:large_scale_deep_learning}

深度学习基于联结主义的哲学：虽然单个的生物神经元或者机器学习模型中的单个特征并不
% See https://en.wikipedia.org/wiki/Connectionism
智能，大量神经元或者特征的共同作用可以表现出智能行为。强调这一事实非常重要，神经
元的数量必须是\textbf{大量的}。主导改进神经网络准确性和任务复杂度~——~那些从 80 年
代至今解决的任务~——~的一个关键因素，是我们使用的网络规模上引人注目的增长。正如我
们在~\ref{subsec:increasing_model_sizes} 节中看到的，网络规模在过去三十年间以指数
方式增长，然而人工神经网络仅仅和昆虫的神经系统一样大。

因为神经网络的规模是非常重要的，深度学习需要高性能的硬件和软件基础。

\subsection{快速的 CPU 实现}
\label{subsec:fast_cpu_implementations}

传统上，神经网络利用一台机器的 CPU 训练。今天，这种方法被认为是不够的。我们现在主
要使用 GPU 运算或者多机联网的 CPU。在移到这些昂贵的装置前，研究人员努力证明 CPU
无法完成神经网络需要的很高的计算工作量。

如何执行高效数值计算的 CPU 代码的描述超出了这本书的范围，但我们在这里强调，仔细为
特定的 CPU 家族的实现可以产生较大的改进。例如，在 2011 年，最好的 CPU 在使用定点
运算时，运行神经网络的工作量要快于使用浮点运算。通过创造一个仔细调试的定点实
现，\citet{37631} 取得了超过一个强大的浮点系统的三倍提速。每个新的 CPU 型号有不同
的性能特点，所以有时浮点实现也能够更快。重要的原则是，对数值计算程序的仔细的专门
处理，可以获得很大的回报。其他策略，除了选择是否使用定点或浮点，还包括优化数据结
构，以避免高速缓存块未命中（cache misses）和使用矢量指令。许多机器学习的研究人员
忽视了这些实现细节，但当具体实现的性能限制模型的大小时，模型的准确性受到影响。

\subsection{GPU 实现}
\label{subsec:gpu_implementations}

大部分现代神经网络的实现基于图形处理单元。图形处理单元（GPU）是最初为图形应用开发
的专门的硬件组件。视频游戏系统的消费者市场刺激了图形处理硬件的发展。好的视频游戏
系统所需的性能特性结果发现对神经网络同样有益。

视频游戏渲染需要并行快速地执行很多操作。角色和环境模型以三维的顶点坐标列表的形式
指定。图形卡必须执行许多顶点的矩阵乘除，同时并行地将这些三维坐标转换到二维的屏幕
坐标。图形卡必须在每个像素执行许多计算，同时并行地确定每个像素的颜色。在这两种情
况下，计算都相当简单，和一个 CPU 通常遇到的计算工作量相比，并不涉及大量的分支。例
如，在相同的刚性物体中的每个顶点会被相同的矩阵相乘；没有必要每个顶点用 {\serif
  if} 声明求值来确定乘以哪个矩阵。这些计算彼此间也完全无关，这样可以很容易并行化。
这些计算也涉及处理大量的内存缓存，包括描述每个渲染物体纹理（颜色图案）的位图。结
合在一起，这导致图形卡被设计成具有高度的并行性和高内存带宽，其代价是相对于传
统CPU 有一个更低的时钟速度和更少的分支能力。

神经网络算法需要和上面描述的实时图形算法相同的性能特性。神经网络通常涉及数量庞大
的参数、激活值和梯度值的缓存，其中每个都必须在每一步的训练中被完整更新。这些缓存
大到足够超出传统的桌面计算机的高速缓冲区（cache）而崩溃。GPU，由于它们的高内存带
宽，提供了一个超过 CPU 的不可抗拒的优势。神经网络训练算法通常不涉及很多分支或者复
杂控制，所以它们适用于 GPU 硬件。既然神经网络能被分成多个单独的“神经元”，这些“神
经元”在相同层中能彼此独立地被处理，神经网络很容易从 GPU 计算的并行性获益。

GPU 硬件最初是如此特殊，它只能用于图形任务。随着时间的过去，GPU 硬件变得更灵活了，
允许定制子程序用于变换定点坐标或者分配颜色给像素。原则上，没有要求这些像素值要实
际基于渲染任务。这些 GPU 能通过将计算所得的输出写入到像素值的缓存而被用于特殊的计
算。\citet{1575717} 在一块 GPU 上实现了一个两层的全连接神经网络并宣告了一个超过他
们基于 CPU 基线的三倍提速。此后不久，\citet{chellapilla:inria-00112631} 演示了同
样的技术能被用于加速有监督的卷积网络。

用于神经网络训练的图形卡在\textbf{通用 GPU}发明后得到爆炸性的普及。这些 GP-GPU 能
够执行任意代码，不仅仅是渲染子程序。NVIDIA 的 CUDA 可编程语言提供了一种类 C 语言
的方式编写任意代码。有了它们相对方便的编程模型、大规模并行结构和高内存带宽，现
在GP-GPU 提供了一个用于神经网络编程的理想平台。这一平台在面世之后迅速被深度学习研
究人员采用 \citep{Raina:2009:LDU:1553374.1553486,Cireşan2012333}。

为 GP-GPU 编写有效的代码仍然是一项困难的任务，最好留给专家。在 GPU 上获取好的性能
需要的技术和在 CPU 上使用的非常不同。例如，好的基于 CPU 的代码通常被设计为尽可能
从高速缓冲区（cache）读取信息。在 GPU 上，大部分可写的内存位置并没有被高速缓冲，
所以它其实可以更快来两次计算相同值，而不是一次计算然后从内存读回。GPU 代码本质上
也是多线程的，不同的线程必须相互间周密协调。例如，如果内存操作可以\textbf{被合
  并}（\textit{coalesced}），它们可以被更快执行。合并的读写发生在几个线程的每一个
能同时读或写一个它们需要值的时候，作为单次内存处理的一部分。不同的 GPU 模型能够合
并不同类型的读写模式。通常，在 $n$ 个线程中，线程 $i$ 获取内存的 $i + j$ 字节
时~——~其中 $j$ 是 $2$ 的某个倍数~——~，内存操作很容易合并。确切的规格随着 GPU 型号
差异而不同。另一个普遍的对 GPU 的考虑是确保一组中的每一个线程同时执行相同的指令。
这意味着在 GPU 上分支会很困难。线程被分进小的组，称为\emph{\gls{warps}}。一
个\gls{warps}中的每个线程在每个周期中执行相同的指令，所以如果在同一
个\gls{warps}中的不同线程需要执行不同的代码路径，这些不同的代码路径必须按次序遍历，
而不是以平行方式。

由于编写高性能 GPU 代码的困难性，研究人员应该构造他们的工作流程来避免为了测试新的
模型或算法必须编写新的 GPU 代码。通常，可以通过构建一个高性能运算~——~例如卷积和矩
阵乘法~——~的软件库，然后以调用这些运算库的形式指定模型。例如，机器学习
库 Pylearn2 \citep{journals/corr/GoodfellowWLDMPBBB13} 以调用 Theano
\citep{bergstra-proc-scipy-2010,DBLP:journals/corr/abs-1211-5590} 和
cuda-convnet \citep{krizhevsky2010convolutional} 的形式指定所有它的机器学习算法，
它们提供了这些高性能运算。这种分解方法也可以缓解多种硬件支持。例如，同样
的 Theano 程序可以运行在 CPU 或 GPU上，不需要改变任何自身对 Theano 的调用。其它库，
例如 TensorFlow \citep{tensorflow} 和 Torch \citep{collobert2011torch7} 提供了类
似的特性。

\subsection{大规模分布式实现}
\label{subsec:large_scale_distributed_implementations}

在许多情况下，在一台机器上可用的计算资源是不够的。因此，我们想要在多台机器间分配
训练和推断结果的工作负担。

分配推断结果的工作简单，因为每个我们想要处理的能被单独的机器运行。这称
为\textbf{数据并行}。

也可能取得\textbf{模型并行}，其中多台机器在单个数据点上一起工作，每台机器运行模型
的不同部分。这对于推断结果和训练都很灵活。

训练期间的数据并行处理有点难。我们可以增加用于单个 \gls{SGD} 步骤的小批量数据的规
模，但是通常我们在性能优化方面得到更少的线性回报。允许多台机器并行地计算多个梯度
下降步骤会更好。不幸的是，梯度下降的标准定义是一个完全按次序的算法：在步骤 $t$ 的
梯度是一个参数由步骤 $t-1$ 产生的函数。

这可以通过使用\textbf{异步随机梯度下降}\citep{NIPS2011_4390}来解决。在这种方法中，
% FIXME: cannot find (Bengio et al., 2001)
几个处理器核心共享表示参数的内存。每个核心在不锁定的情况下读取参数，计算一个梯度，
然后在不锁定的情况下递增这些参数。这减少了每个梯度下降步骤得出改进的平均量，因为
一些核心复写了彼此的进度，但是那些步骤产生的增加的速率使得学习进程整体上更
快。\citet{NIPS2012_4687}首创了这用于梯度下降的无锁方法的多机实现，其中参数由一台
参数服务器管理，而不是存储在共享内存中。分布式异步梯度下降仍然是训练大规模深度网
络的主要策略，并且被大多数行业内主要的深度学习群体使用\citep{186212,
  DBLP:journals/corr/WuYSDS15}。大学院校的深度学习研究人员通常不能负担得起同样规
模的分布式学习系统，但是一些研究人员已经专注于如何用在大学环境中可取得的相对低成
本的硬件构建分布式网络\citep{coates2013deep}。
