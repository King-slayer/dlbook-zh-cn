\chapter{应用}
\label{ch:applications}

在这一章，我们描述如何使用深度学习来解决在计算机视觉、语音识别、自然语言处理和其
它商业性领域的应用。首先，我们讨论最重要的人工智能应用所需的大规模神经网络实现。
接下来，我们回顾几个深度学习已经被用于解决问题的特殊应用领域。虽然深度学习的一个
目标是设计能够解决各种各样任务的算法，到目前为止还需要一定程度的专业化。例如，视
觉任务每个样本需要处理大量输入特征（像素）。语言任务每个输入特征需要对大量可能值
（词汇表中的单词）建模。

\section{大规模深度学习}
\label{sec:large_scale_deep_learning}

深度学习基于联结主义的哲学：虽然单个的生物神经元或者机器学习模型中的单个特征并不
% See https://en.wikipedia.org/wiki/Connectionism
智能，大量神经元或者特征的共同作用可以表现出智能行为。强调这一事实非常重要，神经
元的数量必须是\emph{大量的}。主导改进神经网络准确性和任务复杂度~——~那些从 80 年
代至今解决的任务~——~的一个关键因素，是我们使用的网络规模上引人注目的增长。正如我
们在~\ref{subsec:increasing_model_sizes} 节中看到的，网络规模在过去三十年间以指
数方式增长，然而人工神经网络仅仅和昆虫的神经系统一样大。

因为神经网络的规模是非常重要的，深度学习需要高性能的硬件和软件基础。

\subsection{快速的 CPU 实现}
\label{subsec:fast_cpu_implementations}

传统上，神经网络利用一台机器的 CPU 训练。今天，这种方法被认为是不够的。我们现在
主要使用 GPU 运算或者多机联网的 CPU。在移到这些昂贵的装置前，研究人员努力证明
CPU 无法完成神经网络需要的很高的计算工作量。

如何执行高效数值计算的 CPU 代码的描述超出了这本书的范围，但我们在这里强调，仔细
为特定的 CPU 家族的实现可以产生较大的改进。例如，在 2011 年，最好的 CPU 在使用定
点运算时，运行神经网络的工作量要快于使用浮点运算。通过创造一个仔细调试的定点实现，
\citet{37631} 取得了超过一个强大的浮点系统的三倍提速。每个新的 CPU 型号有不同的
性能特点，所以有时浮点实现也能够更快。重要的原则是，对数值计算程序的仔细的专门处
理，可以获得很大的回报。其他策略，除了选择是否使用定点或浮点，还包括优化数据结构，
以避免高速缓存块未命中（cache misses）和使用矢量指令。许多机器学习的研究人员忽视
了这些实现细节，但当具体实现的性能限制模型的大小时，模型的准确性受到影响。

\subsection{GPU 实现}
\label{subsec:gpu_implementations}

大部分现代神经网络的实现基于图形处理单元。图形处理单元（GPU）是最初为图形应用开
发的专门的硬件组件。视频游戏系统的消费者市场刺激了图形处理硬件的发展。好的视频游
戏系统所需的性能特性结果发现对神经网络同样有益。

视频游戏渲染需要并行快速地执行很多操作。角色和环境模型以三维的顶点坐标列表的形式
指定。图形卡必须执行许多顶点的矩阵乘除，同时并行地将这些三维坐标转换到二维的屏幕
坐标。图形卡必须在每个像素执行许多计算，同时并行地确定每个像素的颜色。在这两种情
况下，计算都相当简单，和一个 CPU 通常遇到的计算工作量相比，并不涉及大量的分支。
例如，在相同的刚性物体中的每个顶点会被相同的矩阵相乘；没有必要每个顶点用 {\serif
  if} 声明求值来确定乘以哪个矩阵。这些计算彼此间也完全无关，这样可以很容易并行化。
这些计算也涉及处理大量的内存缓存，包括描述每个渲染物体纹理（颜色图案）的位图。结
合在一起，这导致图形卡被设计成具有高度的并行性和高内存带宽，其代价是相对于传统
CPU 有一个更低的时钟速度和更少的分支能力。

神经网络算法需要和上面描述的实时图形算法相同的性能特性。神经网络通常涉及数量庞大
的参数、激活值和梯度值的缓存，其中每个都必须在每一步的训练中被完整更新。这些缓存
大到足够超出传统的桌面计算机的高速缓冲区（cache）而崩溃。GPU，由于它们的高内存带
宽，提供了一个超过 CPU 的不可抗拒的优势。神经网络训练算法通常不涉及很多分支或者
复杂控制，所以它们适用于 GPU 硬件。既然神经网络能被分成多个单独的“神经元”，这些
“神经元”在相同层中能彼此独立地被处理，神经网络很容易从 GPU 计算的并行性获益。

GPU 硬件最初是如此特殊，它只能用于图形任务。随着时间的过去，GPU 硬件变得更灵活了，
允许定制子程序用于变换定点坐标或者分配颜色给像素。原则上，没有要求这些像素值要实
际基于渲染任务。这些 GPU 能通过将计算所得的输出写入到像素值的缓存而被用于特殊的
计算。\citet{1575717} 在一块 GPU 上实现了一个两层的全连接神经网络并宣告了一个超
过他们基于 CPU 基线的三倍提速。此后不久，\citet{chellapilla:inria-00112631} 演示
了同样的技术能被用于加速有监督的卷积网络。

用于神经网络训练的图形卡在\emph{通用 GPU}发明后得到爆炸性的普及。这些 GP-GPU 能
够执行任意代码，不仅仅是渲染子程序。NVIDIA 的 CUDA 可编程语言提供了一种类 C 语言
的方式编写任意代码。有了它们相对方便的编程模型、大规模并行结构和高内存带宽，现在
GP-GPU 提供了一个用于神经网络编程的理想平台。这一平台在面世之后迅速被深度学习研
究人员采用 \citep{Raina:2009:LDU:1553374.1553486,Cireşan2012333}。

为 GP-GPU 编写有效的代码仍然是一项困难的任务，最好留给专家。在 GPU 上获取好的性
能需要的技术和在 CPU 上使用的非常不同。例如，好的基于 CPU 的代码通常被设计为尽可
能从高速缓冲区（cache）读取信息。在 GPU 上，大部分可写的内存位置并没有被高速缓冲，
所以它其实可以更快来两次计算相同值，而不是一次计算然后从内存读回。GPU 代码本质上
也是多线程的，不同的线程必须相互间周密协调。例如，如果内存操作可以被\emph{合并}
（\textit{coalesced}），它们可以被更快执行。合并的读写发生在几个线程的每一个能同
时读或写一个它们需要值的时候，作为单次内存处理的一部分。不同的 GPU 模型能够合并
不同类型的读写模式。通常，在 $n$ 个线程中，线程 $i$ 获取内存的 $i + j$ 字节时~——~其
中 $j$ 是 $2$ 的某个倍数~——~，内存操作很容易合并。确切的规格随着 GPU 型号差异而
不同。另一个普遍的对 GPU 的考虑是确保一组中的每一个线程同时执行相同的指令。这意
味着在 GPU 上分支会很困难。线程被分进小的组，称为\emph{warps}。一个 warp 中的每
个线程在每个周期中执行相同的指令，所以如果在同一个 warp 中的不同线程需要执行不同
的代码路径，这些不同的代码路径必须按次序遍历，而不是以平行方式。

由于编写高性能 GPU 代码的困难性，研究人员应该构造他们的工作流程来避免为了测试新
的模型或算法必须编写新的 GPU 代码。通常，可以通过构建一个高性能运算~——~例如卷积
和矩阵乘法~——~的软件库，然后以调用这些运算库的形式指定模型。例如，机器学习库
Pylearn2 \citep{journals/corr/GoodfellowWLDMPBBB13} 以调用 Theano
\citep{bergstra-proc-scipy-2010,DBLP:journals/corr/abs-1211-5590} 和
cuda-convnet \citep{krizhevsky2010convolutional} 的形式指定所有它的机器学习算法，
它们提供了这些高性能运算。这种分解方法也可以缓解多种硬件支持。例如，同样的
Theano 程序可以运行在CPU 或 GPU上，不需要改变任何自身对 Theano 的调用。其它库，
例如 TensorFlow \citep{tensorflow} 和 Torch \citep{collobert2011torch7} 提供了类
似的特性。
